#!/usr/bin/env python3 """ Clean Multi-Service Parallel Architecture All mock code removed - ready for real implementation """ import asyncio import aiohttp import json import time from typing import List, Dict, Optional, Tuple from dataclasses import dataclass from concurrent.futures import ThreadPoolExecutor, as_completed import numpy as np from queue import Queue import threading @dataclass class ServiceTask: """Task definition for parallel service processing""" task_id: str service_type: str claim_text: str atomic_claim_id: str priority: int = 1 # 1=high, 2=medium, 3=low dependencies: List[str] = None # Task dependencies def __post_init__(self): if self.dependencies is None: self.dependencies = [] @dataclass class ServiceResult: """Result from service processing""" task_id: str service_type: str success: bool data: Dict processing_time: float error: Optional[str] = None class ParallelServiceOrchestrator: """ Orchestrates multiple services running in parallel Services: Decomposer, Brave Search, MiniCheck, Evidence Aggregator """ def __init__(self): # Service configurations self.services = { 'decomposer': { 'url': 'http://localhost:8006', 'endpoint': '/decompose', 'timeout': 30, 'max_concurrent': 5 }, 'brave_search': { 'url': 'http://localhost:8010', # Real service to be implemented 'endpoint': '/search', 'timeout': 15, 'max_concurrent': 20 }, 'minicheck': { 'url': 'http://localhost:8011', # Real service to be implemented 'endpoint': '/check', 'timeout': 10, 'max_concurrent': 15 }, 'evidence_aggregator': { 'url': 'http://localhost:8012', # Real service to be implemented 'endpoint': '/aggregate', 'timeout': 5, 'max_concurrent': 10 } } # Task queues for each service self.task_queues = {service: Queue() for service in self.services} # Result tracking self.results = {} self.task_status = {} # Performance metrics self.metrics = { 'total_tasks': 0, 'completed_tasks': 0, 'failed_tasks': 0, 'total_time': 0.0, 'service_performance': {service: {} for service in self.services} } # Thread pool for parallel processing self.executor = ThreadPoolExecutor(max_workers=50) # Service health status self.service_health = {service: True for service in self.services} async def process_claim_parallel(self, claim: str) -> Dict: """ Main entry point for parallel claim processing """ start_time = time.time() try: print(" Starting Multi-Service Parallel Processing") # Step 1: Decompose claim (must run first) print(" Step 1: Decomposing claim...") decompose_result = await self._call_service('decomposer', { 'claim': claim, 'language': 'vietnamese', 'max_sub_claims': 20 }) if not decompose_result['success']: return self._error_result(claim, "Decomposition failed", time.time() - start_time) atomic_claims = decompose_result.get('sub_claims', []) # Step 2: Create parallel tasks for all services print(" Step 2: Creating parallel tasks...") tasks = self._create_parallel_tasks(claim, atomic_claims) # Step 3: Execute all tasks in parallel print(" Step 3: Executing parallel tasks...") parallel_results = await self._execute_parallel_tasks(tasks) # Step 4: Aggregate and format results print(" Step 4: Aggregating results...") final_result = self._aggregate_results(claim, decompose_result, parallel_results) total_time = time.time() - start_time result = { "success": True, "claim": claim, "processing_time": total_time, "architecture": "multi_service_parallel", "services": list(self.services.keys()), "decomposition": { "atomic_claims": len(atomic_claims), "processing_time": decompose_result['processing_time'] }, "parallel_processing": { "total_tasks": len(tasks), "completed_tasks": len([r for r in parallel_results if r['success']]), "failed_tasks": len([r for r in parallel_results if not r['success']]), "parallel_time": total_time - decompose_result['processing_time'] }, "results": final_result, "performance": self._calculate_performance_metrics(final_result, total_time) } # Update metrics self._update_metrics(result) return result except Exception as e: return self._error_result(claim, str(e), time.time() - start_time) def _create_parallel_tasks(self, claim: str, atomic_claims: List[Dict]) -> List[ServiceTask]: """Create parallel tasks for all services""" tasks = [] for i, atomic_claim in enumerate(atomic_claims): claim_id = f"claim_{i}_{int(time.time() * 1000)}" claim_text = atomic_claim.get('text', '') # Brave Search task tasks.append(ServiceTask( task_id=f"{claim_id}_brave_search", service_type="brave_search", claim_text=claim_text, atomic_claim_id=claim_id, priority=1, # High priority dependencies=[] )) # MiniCheck task (can run parallel with search) tasks.append(ServiceTask( task_id=f"{claim_id}_minicheck", service_type="minicheck", claim_text=claim_text, atomic_claim_id=claim_id, priority=1, # High priority dependencies=[] )) # Evidence Aggregation task (depends on search results) tasks.append(ServiceTask( task_id=f"{claim_id}_aggregate", service_type="evidence_aggregator", claim_text=claim_text, atomic_claim_id=claim_id, priority=2, # Medium priority dependencies=[f"{claim_id}_brave_search"] )) return tasks async def _execute_parallel_tasks(self, tasks: List[ServiceTask]) -> List[ServiceResult]: """Execute all tasks in parallel using dependency management""" # Group tasks by service type service_tasks = {service: [] for service in self.services} for task in tasks: if task.service_type not in service_tasks: service_tasks[task.service_type] = [] service_tasks[task.service_type].append(task) # Execute tasks in waves based on dependencies all_results = [] # Wave 1: Tasks with no dependencies (decomposer already done) wave1_tasks = [task for task in tasks if not task.dependencies] # Wave 2: Tasks that depend on Wave 1 wave2_tasks = [task for task in tasks if task.dependencies] # Execute Wave 1 if wave1_tasks: wave1_results = await self._execute_task_wave(wave1_tasks) all_results.extend(wave1_results) # Execute Wave 2 (after Wave 1 completes) if wave2_tasks: wave2_results = await self._execute_task_wave(wave2_tasks) all_results.extend(wave2_results) return all_results async def _execute_task_wave(self, tasks: List[ServiceTask]) -> List[ServiceResult]: """Execute a wave of tasks in parallel""" # Group by service type service_groups = {} for task in tasks: if task.service_type not in service_groups: service_groups[task.service_type] = [] service_groups[task.service_type].append(task) # Execute each service group concurrently group_results = [] for service_type, service_tasks in service_groups.items(): # Execute tasks for this service in parallel service_results = await self._execute_service_tasks(service_type, service_tasks) group_results.extend(service_results) return group_results async def _execute_service_tasks(self, service_type: str, tasks: List[ServiceTask]) -> List[ServiceResult]: """Execute tasks for a specific service in parallel""" service_config = self.services[service_type] max_concurrent = service_config['max_concurrent'] # Create semaphore for concurrency control semaphore = asyncio.Semaphore(max_concurrent) async def execute_single_task(task: ServiceTask) -> ServiceResult: async with semaphore: return await self._call_service(task.service_type, { 'claim': task.claim_text, 'task_id': task.task_id, 'atomic_claim_id': task.atomic_claim_id }) # Execute all tasks for this service concurrently results = await asyncio.gather(*[execute_single_task(task) for task in tasks], return_exceptions=True) # Convert exceptions to error results service_results = [] for i, result in enumerate(results): if isinstance(result, Exception): service_results.append(ServiceResult( task_id=tasks[i].task_id, service_type=service_type, success=False, data={}, processing_time=0.0, error=str(result) )) else: service_results.append(result) return service_results async def _call_service(self, service_type: str, payload: Dict) -> ServiceResult: """Call a specific service""" start_time = time.time() service_config = self.services[service_type] try: async with aiohttp.ClientSession() as session: url = f"{service_config['url']}{service_config['endpoint']}" async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=service_config['timeout'])) as response: processing_time = time.time() - start_time if response.status == 200: data = await response.json() return ServiceResult( task_id=payload.get('task_id', 'unknown'), service_type=service_type, success=True, data=data, processing_time=processing_time ) else: return ServiceResult( task_id=payload.get('task_id', 'unknown'), service_type=service_type, success=False, data={}, processing_time=processing_time, error=f"HTTP {response.status}" ) except Exception as e: return ServiceResult( task_id=payload.get('task_id', 'unknown'), service_type=service_type, success=False, data={}, processing_time=time.time() - start_time, error=str(e) ) def _aggregate_results(self, claim: str, decompose_result: ServiceResult, parallel_results: List[ServiceResult]) -> Dict: """Aggregate results from all services""" # Group results by atomic claim atomic_results = {} for result in parallel_results: if result.success: atomic_claim_id = result.data.get('atomic_claim_id', 'unknown') if atomic_claim_id not in atomic_results: atomic_results[atomic_claim_id] = {} atomic_results[atomic_claim_id][result.service_type] = result.data # Format final results formatted_results = [] atomic_claims = decompose_result.get('sub_claims', []) for i, atomic_claim in enumerate(atomic_claims): claim_id = f"claim_{i}_{int(time.time() * 1000)}" result_data = { "atomic_claim": atomic_claim, "claim_id": claim_id, "services": atomic_results.get(claim_id, {}), "overall_status": "completed" if claim_id in atomic_results else "failed" } formatted_results.append(result_data) return formatted_results def _calculate_performance_metrics(self, results: Dict, total_time: float) -> Dict: """Calculate performance metrics""" atomic_claims = results.get('results', []) total_atomic = len(atomic_claims) # Service performance service_performance = {} for atomic_result in atomic_claims: services = atomic_result.get('services', {}) for service_type, service_data in services.items(): if service_type not in service_performance: service_performance[service_type] = { 'success_count': 0, 'total_count': 0, 'avg_time': 0.0 } service_performance[service_type]['total_count'] += 1 if 'processing_time' in service_data: service_performance[service_type]['avg_time'] += service_data['processing_time'] if 'success' in service_data and service_data['success']: service_performance[service_type]['success_count'] += 1 # Calculate averages for service_type in service_performance: perf = service_performance[service_type] if perf['total_count'] > 0: perf['success_rate'] = perf['success_count'] / perf['total_count'] perf['avg_time'] = perf['avg_time'] / perf['total_count'] else: perf['success_rate'] = 0.0 return { "total_atomic_claims": total_atomic, "processing_time": total_time, "throughput": total_atomic / total_time if total_time > 0 else 0, "service_performance": service_performance } def _update_metrics(self, result: Dict): """Update global metrics""" self.metrics['total_tasks'] += result.get('parallel_processing', {}).get('total_tasks', 0) self.metrics['completed_tasks'] += result.get('parallel_processing', {}).get('completed_tasks', 0) self.metrics['failed_tasks'] += result.get('parallel_processing', {}).get('failed_tasks', 0) self.metrics['total_time'] += result.get('processing_time', 0) def get_metrics(self) -> Dict: """Get current metrics""" return self.metrics def _error_result(self, claim: str, error: str, processing_time: float) -> Dict: """Error result format""" return { "success": False, "claim": claim, "error": error, "processing_time": processing_time, "architecture": "multi_service_parallel" } async def test_clean_parallel_architecture(): """Test the clean multi-service parallel architecture""" print("TESTING CLEAN MULTI-SERVICE PARALLEL ARCHITECTURE") print("=" * 80) print("Services: Decomposer → Brave Search → MiniCheck → Evidence Aggregator") print("Architecture: Full parallel processing with dependency management") print(" All mock code removed - Ready for real implementation") print("=" * 80) # Create orchestrator orchestrator = ParallelServiceOrchestrator() # Test claims test_claims = [ "Hà Nội là thủ đô của Việt Nam và có dân số hơn 8 triệu người", "Chủ tịch Hồ Chí Minh đã tuyên bố độc lập Việt Nam vào ngày 2 tháng 9 năm 1945 tại Hà Nội", "Việt Nam đạt tăng trưởng kinh tế 6.7% trong năm 2023, triển khai công nghệ 5G, và thu hút FDI 28 tỷ đô" ] results = [] for i, claim in enumerate(test_claims, 1): print(f"\n Test {i}: Parallel Framework Test") print(f"Claim: {claim}") print("-" * 60) result = await orchestrator.process_claim_parallel(claim) if result['success']: print(f" Status: SUCCESS") print(f"Atomic Claims: {result['decomposition']['atomic_claims']}") print(f"Total Tasks: {result['parallel_processing']['total_tasks']}") print(f"Completed Tasks: {result['parallel_processing']['completed_tasks']}") print(f"Failed Tasks: {result['parallel_processing']['failed_tasks']}") print(f"Total Time: {result['processing_time']:.2f}s") print(f"Parallel Time: {result['parallel_processing']['parallel_time']:.2f}s") print(f"Throughput: {result['performance']['throughput']:.2f} claims/sec") # Service performance service_perf = result['performance']['service_performance'] print(f"\n Service Performance:") for service, perf in service_perf.items(): print(f" {service}: {perf.get('success_rate', 0):.1%} success, {perf.get('avg_time', 0):.2f}s avg") # Results summary atomic_results = result['results'] completed = len([r for r in atomic_results if r['overall_status'] == 'completed']) print(f"\n Results Summary:") print(f" Completed Atomic Claims: {completed}/{len(atomic_results)}") else: print(f" Status: FAILED") print(f"Error: {result.get('error', 'Unknown error')}") results.append(result) # Final metrics print("\n" + "=" * 80) print("PARALLEL FRAMEWORK METRICS") print("=" * 80) metrics = orchestrator.get_metrics() print(f"Total Tasks Processed: {metrics['total_tasks']}") print(f"Completed Tasks: {metrics['completed_tasks']}") print(f"Failed Tasks: {metrics['failed_tasks']}") print(f"Success Rate: {metrics['completed_tasks'] / metrics['total_tasks'] * 100:.1f}" if metrics['total_tasks'] > 0 else "Success Rate: 0.0%") print(f"Total Processing Time: {metrics['total_time']:.2f}s") print("\n NEXT STEPS:") print("1. Implement Brave Search service (port 8010)") print("2. Implement MiniCheck service (port 8011)") print("3. Implement Evidence Aggregator (port 8012)") print("4. Test with real services") return results if __name__ == "__main__": asyncio.run(test_clean_parallel_architecture()) 