#!/usr/bin/env python3 """ LLM Service using HuggingFace Transformers Direct Qwen2.5-7B integration without Ollama """ import torch from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline import asyncio import json import time from typing import Dict, List, Optional from fastapi import FastAPI, HTTPException from pydantic import BaseModel import uvicorn class LLMRequest(BaseModel): prompt: str model: str = "Qwen/Qwen2.5-7B" temperature: float = 0.1 max_tokens: int = 1000 do_sample: bool = True class LLMResponse(BaseModel): response: str model: str processing_time: float success: bool error: Optional[str] = None class QwenService: def __init__(self, model_name: str = "Qwen/Qwen2.5-7B", cache_dir: str = "D:/huggingface_cache"): self.model_name = model_name self.cache_dir = cache_dir self.device = "cuda" if torch.cuda.is_available() else "cpu" self.tokenizer = None self.model = None self.pipeline = None self.loaded = False async def load_model(self): """Load Qwen2.5-7B model""" if self.loaded: return True try: print(f"Loading {self.model_name} on {self.device.upper()}...") print(f"Cache directory: {self.cache_dir}") # Load tokenizer self.tokenizer = AutoTokenizer.from_pretrained( self.model_name, cache_dir=self.cache_dir, trust_remote_code=True ) # Load model self.model = AutoModelForCausalLM.from_pretrained( self.model_name, cache_dir=self.cache_dir, torch_dtype=torch.float16 if self.device == "cuda" else torch.float32, device_map="auto" if self.device == "cuda" else None, trust_remote_code=True ) # Create pipeline self.pipeline = pipeline( "text-generation", model=self.model, tokenizer=self.tokenizer, device=0 if self.device == "cuda" else -1, torch_dtype=torch.float16 if self.device == "cuda" else torch.float32 ) self.loaded = True print(f"{self.model_name} loaded successfully!") return True except Exception as e: print(f"ERROR: Failed to load model: {e}") return False async def generate_response(self, prompt: str, **kwargs) -> LLMResponse: """Generate response using Qwen2.5-7B""" if not self.loaded: await self.load_model() if not self.loaded: return LLMResponse( response="", model=self.model_name, processing_time=0.0, success=False, error="Model not loaded" ) start_time = time.time() try: # Prepare generation parameters generation_params = { "max_new_tokens": kwargs.get("max_tokens", 1000), "temperature": kwargs.get("temperature", 0.1), "do_sample": kwargs.get("do_sample", True), "pad_token_id": self.tokenizer.eos_token_id, "eos_token_id": self.tokenizer.eos_token_id, "return_full_text": False } # Generate response result = self.pipeline( prompt, **generation_params ) processing_time = time.time() - start_time # Extract generated text if isinstance(result, list) and len(result) > 0: generated_text = result[0]["generated_text"] # Remove the original prompt from the response if generated_text.startswith(prompt): response_text = generated_text[len(prompt):].strip() else: response_text = generated_text.strip() else: response_text = str(result).strip() return LLMResponse( response=response_text, model=self.model_name, processing_time=processing_time, success=True ) except Exception as e: processing_time = time.time() - start_time return LLMResponse( response="", model=self.model_name, processing_time=processing_time, success=False, error=str(e) ) # FastAPI App app = FastAPI(title="LLM Service", description="Qwen2.5-7B Service for Vietnamese Fact Checker") # Global service instance llm_service = QwenService() @app.on_event("startup") async def startup_event(): """Load model on startup""" await llm_service.load_model() @app.get("/") async def root(): """Health check endpoint""" return { "status": "healthy", "model": llm_service.model_name, "device": llm_service.device, "loaded": llm_service.loaded, "cache_dir": llm_service.cache_dir, "service": "Qwen2.5-7B LLM Service" } @app.get("/health") async def health(): """Detailed health check""" return { "status": "healthy" if llm_service.loaded else "loading", "model_loaded": llm_service.loaded, "model": llm_service.model_name, "device": llm_service.device, "cache_dir": llm_service.cache_dir, "gpu_available": torch.cuda.is_available(), "gpu_memory": torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0 } @app.post("/generate", response_model=LLMResponse) async def generate(request: LLMRequest): """Generate text using Qwen2.5-7B""" result = await llm_service.generate_response( prompt=request.prompt, model=request.model, temperature=request.temperature, max_tokens=request.max_tokens, do_sample=request.do_sample ) if not result.success: raise HTTPException(status_code=500, detail=result.error) return result @app.post("/test") async def test_llm(): """Test LLM with a simple prompt""" test_prompt = "Hello! Please respond with 'Qwen2.5-7B is working correctly.'" result = await llm_service.generate_response(test_prompt) return { "test_passed": result.success, "response": result.response, "processing_time": result.processing_time, "model": result.model } @app.post("/vietnamese-test") async def vietnamese_test(): """Test LLM with Vietnamese""" test_prompt = "Xin chào! Hãy trả lời bằng tiếng Việt: 'Qwen2.5-7B đang hoạt động tốt.'" result = await llm_service.generate_response(test_prompt) return { "test_passed": result.success, "response": result.response, "processing_time": result.processing_time, "model": result.model } if __name__ == "__main__": print("Starting Qwen2.5-7B LLM Service...") print("Service will be available at: http://localhost:8009") print("Model: Qwen/Qwen2.5-7B") print("Cache directory: D:/huggingface_cache") print("Device: CUDA" if torch.cuda.is_available() else "CPU") uvicorn.run(app, host="0.0.0.0", port=8009) 