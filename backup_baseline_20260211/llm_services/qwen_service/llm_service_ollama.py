#!/usr/bin/env python3 """ Ollama-based LLM Service for Gemma-2B Uses Ollama as backend for model serving """ import requests import asyncio import json import time from typing import Dict, List, Optional from fastapi import FastAPI, HTTPException from pydantic import BaseModel import uvicorn class LLMRequest(BaseModel): prompt: str model: str = "qwen2:1.5b" temperature: float = 0.1 max_tokens: int = 1000 do_sample: bool = True class LLMResponse(BaseModel): response: str model: str processing_time: float success: bool error: Optional[str] = None class OllamaLLMService: def __init__(self, ollama_url: str = "http://localhost:11434"): self.ollama_url = ollama_url self.loaded = False self.model_name = "qwen2:1.5b" async def check_ollama(self): """Check if Ollama is running""" try: response = requests.get(f"{self.ollama_url}/api/tags", timeout=5) if response.status_code == 200: print(" Ollama is running!") return True else: print(f" Ollama returned: {response.status_code}") return False except Exception as e: print(f" Ollama not accessible: {e}") return False async def check_model(self): """Check if Gemma-2B model is available""" try: response = requests.get(f"{self.ollama_url}/api/tags", timeout=5) if response.status_code == 200: models = response.json().get("models", []) model_names = [model.get("name", "") for model in models] if self.model_name in model_names: print(f" {self.model_name} model is available!") return True else: print(f" {self.model_name} model not found") print(f"Available models: {model_names}") return False return False except Exception as e: print(f" Error checking models: {e}") return False async def load_model(self): """Load/check model availability""" if self.loaded: return True print("Checking Ollama service...") if not await self.check_ollama(): print(" Ollama is not running!") print("Please install and start Ollama:") print("1. Download: https://ollama.com/download") print("2. Install Ollama") print("3. Run: ollama pull gemma2b") print("4. Run: ollama serve") return False if not await self.check_model(): print(f" {self.model_name} model not available!") print(f"Run: ollama pull {self.model_name}") return False self.loaded = True print(f" {self.model_name} ready!") return True async def generate_response(self, prompt: str, **kwargs) -> LLMResponse: """Generate response using Ollama""" if not self.loaded: await self.load_model() if not self.loaded: return LLMResponse( response="", model=self.model_name, processing_time=0.0, success=False, error="Ollama or model not available" ) start_time = time.time() try: # Prepare Ollama API request payload = { "model": self.model_name, "prompt": prompt, "stream": False, "options": { "temperature": kwargs.get("temperature", 0.1), "num_predict": kwargs.get("max_tokens", 1000), "top_p": 0.9, "top_k": 40 } } # Send request to Ollama response = requests.post( f"{self.ollama_url}/api/generate", json=payload, timeout=60 ) processing_time = time.time() - start_time if response.status_code == 200: result = response.json() response_text = result.get("response", "").strip() return LLMResponse( response=response_text, model=self.model_name, processing_time=processing_time, success=True ) else: return LLMResponse( response="", model=self.model_name, processing_time=processing_time, success=False, error=f"Ollama API error: {response.status_code}" ) except Exception as e: processing_time = time.time() - start_time return LLMResponse( response="", model=self.model_name, processing_time=processing_time, success=False, error=str(e) ) # FastAPI App app = FastAPI(title="Ollama LLM Service", description="Ollama-based LLM Service for Gemma-2B") # Global service instance llm_service = OllamaLLMService() @app.on_event("startup") async def startup_event(): """Check Ollama on startup""" await llm_service.load_model() @app.get("/") async def root(): """Health check endpoint""" return { "status": "healthy" if llm_service.loaded else "loading", "model": llm_service.model_name, "ollama_url": llm_service.ollama_url, "loaded": llm_service.loaded, "service": "Ollama LLM Service" } @app.get("/health") async def health(): """Detailed health check""" return { "status": "healthy" if llm_service.loaded else "loading", "model_loaded": llm_service.loaded, "model": llm_service.model_name, "ollama_url": llm_service.ollama_url, "note": "Uses Ollama backend for Gemma-2B" } @app.post("/generate", response_model=LLMResponse) async def generate(request: LLMRequest): """Generate text using Ollama""" result = await llm_service.generate_response( prompt=request.prompt, model=request.model, temperature=request.temperature, max_tokens=request.max_tokens, do_sample=request.do_sample ) if not result.success: raise HTTPException(status_code=500, detail=result.error) return result @app.post("/test") async def test_llm(): """Test LLM with a simple prompt""" test_prompt = "Hello! Please respond with 'Ollama Gemma-2B is working correctly.'" result = await llm_service.generate_response(test_prompt) return { "test_passed": result.success, "response": result.response, "processing_time": result.processing_time, "model": result.model } @app.post("/vietnamese-test") async def vietnamese_test(): """Test LLM with Vietnamese""" test_prompt = "Xin chào! Hãy trả lời bằng tiếng Việt: 'Ollama Gemma-2B đang hoạt động tốt.'" result = await llm_service.generate_response(test_prompt) return { "test_passed": result.success, "response": result.response, "processing_time": result.processing_time, "model": result.model } @app.get("/ollama-status") async def ollama_status(): """Check Ollama and model status""" ollama_running = await llm_service.check_ollama() model_available = await llm_service.check_model() return { "ollama_running": ollama_running, "model_available": model_available, "model": llm_service.model_name, "ollama_url": llm_service.ollama_url } if __name__ == "__main__": print("Starting Ollama LLM Service...") print("Service will be available at: http://localhost:8009") print("Model: qwen2:1.5b (via Ollama)") print("Ollama URL: http://localhost:11434") print("\nRequirements:") print("1. Install Ollama: https://ollama.com/download") print("2. Run: ollama pull qwen2:1.5b") print("3. Run: ollama serve") print("\nStarting service...") uvicorn.run(app, host="0.0.0.0", port=8009) 