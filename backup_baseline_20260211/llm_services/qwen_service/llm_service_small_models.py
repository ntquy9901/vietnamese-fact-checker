#!/usr/bin/env python3 """ Ollama LLM Service with multiple small model options """ import requests import asyncio import json import time from typing import Dict, List, Optional from fastapi import FastAPI, HTTPException from pydantic import BaseModel import uvicorn class LLMRequest(BaseModel): prompt: str model: str = "gemma2b" temperature: float = 0.1 max_tokens: int = 1000 do_sample: bool = True class LLMResponse(BaseModel): response: str model: str processing_time: float success: bool error: Optional[str] = None class SmallModelsService: def __init__(self, ollama_url: str = "http://localhost:11434"): self.ollama_url = ollama_url self.loaded = False self.available_models = [] # Recommended small models self.recommended_models = [ "gemma2b", # 2B - Gemma "llama3.2:1b", # 1B - Llama "qwen2:1.5b", # 1.5B - Qwen (good for Vietnamese) "phi3mini", # 3.8B - Phi "smollm2:135m", # 135M - Very small ] async def check_ollama(self): """Check if Ollama is running""" try: response = requests.get(f"{self.ollama_url}/api/tags", timeout=5) if response.status_code == 200: models = response.json().get("models", []) self.available_models = [model.get("name", "") for model in models] print(" Ollama is running!") print(f"Available models: {self.available_models}") return True else: print(f" Ollama returned: {response.status_code}") return False except Exception as e: print(f" Ollama not accessible: {e}") return False async def load_model(self): """Check model availability""" if self.loaded: return True print("Checking Ollama service...") if not await self.check_ollama(): print(" Ollama is not running!") return False # Check recommended models available_recommended = [] for model in self.recommended_models: if model in self.available_models: available_recommended.append(model) if available_recommended: print(f" Available models: {available_recommended}") self.loaded = True return True else: print(" No recommended models found!") print("Pull one of these models:") for model in self.recommended_models: print(f" ollama pull {model}") return False async def generate_response(self, prompt: str, model: str = None, **kwargs) -> LLMResponse: """Generate response using Ollama""" if not self.loaded: await self.load_model() if not self.loaded: return LLMResponse( response="", model=model or "unknown", processing_time=0.0, success=False, error="No models available" ) # Use provided model or default to first available if not model: model = self.recommended_models[0] start_time = time.time() try: payload = { "model": model, "prompt": prompt, "stream": False, "options": { "temperature": kwargs.get("temperature", 0.1), "num_predict": kwargs.get("max_tokens", 1000), "top_p": 0.9, "top_k": 40 } } response = requests.post( f"{self.ollama_url}/api/generate", json=payload, timeout=60 ) processing_time = time.time() - start_time if response.status_code == 200: result = response.json() response_text = result.get("response", "").strip() return LLMResponse( response=response_text, model=model, processing_time=processing_time, success=True ) else: return LLMResponse( response="", model=model, processing_time=processing_time, success=False, error=f"Ollama API error: {response.status_code}" ) except Exception as e: processing_time = time.time() - start_time return LLMResponse( response="", model=model, processing_time=processing_time, success=False, error=str(e) ) # FastAPI App app = FastAPI(title="Small Models Service", description="Ollama Small Models Service") # Global service instance llm_service = SmallModelsService() @app.on_event("startup") async def startup_event(): """Check models on startup""" await llm_service.load_model() @app.get("/") async def root(): """Health check endpoint""" return { "status": "healthy" if llm_service.loaded else "loading", "available_models": llm_service.available_models, "recommended_models": llm_service.recommended_models, "service": "Small Models Ollama Service" } @app.get("/health") async def health(): """Detailed health check""" return { "status": "healthy" if llm_service.loaded else "loading", "available_models": llm_service.available_models, "recommended_models": llm_service.recommended_models, "ollama_url": llm_service.ollama_url } @app.post("/generate", response_model=LLMResponse) async def generate(request: LLMRequest): """Generate text using small models""" result = await llm_service.generate_response( prompt=request.prompt, model=request.model, temperature=request.temperature, max_tokens=request.max_tokens, do_sample=request.do_sample ) if not result.success: raise HTTPException(status_code=500, detail=result.error) return result @app.post("/test") async def test_llm(): """Test with available model""" test_prompt = "Hello! Please respond with 'Small model is working correctly.'" result = await llm_service.generate_response(test_prompt) return { "test_passed": result.success, "response": result.response, "processing_time": result.processing_time, "model": result.model } @app.post("/vietnamese-test") async def vietnamese_test(): """Test with Vietnamese""" test_prompt = "Xin chào! Hãy trả lời bằng tiếng Việt: 'Small model đang hoạt động tốt.'" result = await llm_service.generate_response(test_prompt) return { "test_passed": result.success, "response": result.response, "processing_time": result.processing_time, "model": result.model } @app.get("/models") async def list_models(): """List available and recommended models""" return { "available": llm_service.available_models, "recommended": llm_service.recommended_models, "ollama_running": await llm_service.check_ollama() } if __name__ == "__main__": print("Starting Small Models Ollama Service...") print("Service will be available at: http://localhost:8009") print("Supported models:") for model in llm_service.recommended_models: print(f" - {model}") print("\nPull commands:") for model in llm_service.recommended_models: print(f" ollama pull {model}") uvicorn.run(app, host="0.0.0.0", port=8009) 